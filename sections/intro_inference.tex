\chapter{Phylogenetic Bayesian models}
\minitoc
\label{sec:phylo_bayes}

\section{Likelihood}

Evolutionary biologists commonly rely on models of sequence change that assume changes at one sequence position have no impact on whether other positions will change. The \gls{likelihood} of the nucleotides or amino acids in an individual column of a multiple sequence alignment can be determined with the pruning algorithm of Felsenstein. This assumption of independence between sites allows the probability of an observed set of aligned sequences at the tips of an evolutionary tree to be expressed as the product over alignment columns of the observed nucleotides or amino acids in those columns. This independence assumption is simplistic, throwing away biological information, and can be shown statistically to be problematic, but permits computationally convenient likelihood-based inference.

\subsection{Alignment}

A concern regarding sequence alignment is that recent research has shown that the outputs of different sequence alignment methods tend to produce different results that are not consistent, and that sequence alignment accuracy degrades sharply with increasing evolutionary divergence.
If datasets are restricted to orthologs from closely related taxa (or to slow-evolving genes), sequence divergence may be less problematic, but if datasets include highly divergent taxa or span functionally divergent paralogous groups, alignment errors become increasingly likely and may cause significant errors in phylogenetic accuracy.

\subsection{Tree topology}

Topology ($\tau$) is considered know.

\subsection{Probability of transitions for a branch}

The branch length $\branchlength\branchexp$ are defined as the expected number of \gls{neutral} substitutions per \acrshort{DNA} site along a branch:
The probability of transition between codons for a given branch $\branch$ and site $\site$ is:
\begin{equation}
\Probmatrix\branchsiteexp = \e^{\branchlength\branchexp \Submatrix\branchsiteexp}
\end{equation}
which are the matrices necessary to compute the \gls{likelihood} of the data ($D$) given the parameters of the model using the pruning algorithm.

\subsection{Equilibrium frequencies}

Verify on Wikipedia

\begin{equation}
\pi\branchsiteexp = \Submatrix\branchsiteexp \pi\branchsiteexp
\end{equation}

\begin{equation}
\pi\branchsiteexp = \Probmatrix\branchsiteexp 
\end{equation}

\subsection{Probability of transitions for a tree}

This section is meant to compute the likelihood of data for the whole tree given parameters of the substitution model.
It will take the example of a illustrative tree given in figure \ref{fig:tree}.
\begin{figure}[htb!]
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=0.6cm and 1.2cm,semithick]
		\tikzstyle{every state}=[]
		
		\node[state] (1) {$S_1$};
		\node[state] (0) [below right=of 1] {$S_0$};
		\node[state] (2) [below left=of 0] {$S_2$};
		\node[state] (5) [right=of 0] {$S_5$};
		\node[state] (3) [above right=of 5] {$S_3$};
		\node[state] (4) [below right=of 5] {$S_4$};
		
		\path[-]
		(1) edge [] node [above] {$\branchlength^{(1)}$} (0)
		(2) edge [] node [above] {$\branchlength^{(2)}$} (0)
		(5) edge [] node [above] {$\branchlength^{(5)}$} (0)
		(5) edge [] node [above] {$\branchlength^{(3)}$} (3)
		(5) edge [] node [above] {$\branchlength^{(4)}$} (4);
		\end{tikzpicture}
	\end{center}
	\caption[Directed acyclic graph of Bayesian network]{Directed acyclic graph of dependencies between variables. Nodes of the directed acyclic graph are the variables, and edges are the functions. Hyper-parameters are depicted in {\color{RED}{red}} circle, random variables in {\color{BLUE}{blue}} circles, and transformed variables in black. Dotted {\color{BLUE}{blue}} line denotes a drawing from a random distribution, and black solid lines denote a function.}
	\label{fig:tree}%
\end{figure}

Likelihood of the data, at site $k$, given the states of the internal nodes is:
\begin{equation*}
p\left(\Data\siteexp|\Subequi^{\treerootexp}, \Probmatrix\branchsiteexp \right) = \prod_{i} \Probmatrix\branchsiteexp_{i}
\end{equation*}

For a data set of protein-coding \acrshort{DNA} sequences, the goal might be to determine the probability of the observed sequence data at the tips of the tree conditional upon the evolutionary model, the tree, and values of parameters in the model. The challenge is that only the data at the tips of the tree are observed whereas the sequence at the root of the tree and the subsequent evolutionary events are not directly observed. Therefore, calculating the \gls{likelihood} of the observed data entails an integration of probability densities over all possible root sequences and all possible subsequent histories of evolutionary events. Such an integration is most often computationally intractable for models of sequence change with dependence among sites.
Likelihood of the data at site $k$, given all possible states of internal node is:
\begin{equation*}
P\left(D| \tau, \mu, \bm{F}^{k}\right) = \sum_{s=1}^{61} \sum_{i}^{61} \prod_{i} \Probmatrix\branchsiteexp_{i}
\end{equation*}

\subsection{Pruning algorithm}


The likelihood of the data $\Data$ can be computed using the pruning algorithm:
\begin{equation}
p\left(\Data\siteexp|\Subequi^{\treerootexp}, \Probmatrix\branchsiteexp \right) = \sum_{\ci=1}^{61} \subequi_{\ci}^{\treerootsiteexp} \pruning\siteexp_{\treeroot} \left( \ci, \Probmatrix\branchsiteexp \right)
\end{equation}
where $\pruning_{\node} \left( \ci \right)$ is computed recursively from the $2$  descendant children $\node_{1}$ and $\node_{2}$ of an internal node $\node$:
\begin{equation}
\pruning\siteexp_{\node} \left( \ci \right) = \left[ \sum_{\cj=1}^{61} \probmatrix^{(\branchnode\left(\node_{1}\right), \site)}_{\itoj} \pruning\siteexp_{\node_{1}} \left( \cj, \Probmatrix\branchsiteexp \right) \right] \cdot \left[ \sum_{\cj=1}^{61} \probmatrix^{(\branchnode\left(\node_{1}\right), \site)}_{\itoj} \pruning\siteexp_{\node_{2}} \left( \cj, \Probmatrix\branchsiteexp \right) \right]
\end{equation}
And if the node $\node$ is a node with no descendant, meaning $\Settaxon$:
\begin{equation}
\pruning\siteexp_{\node}\left( \ci, \Probmatrix\branchsiteexp \right) =
\begin{dcases}
1, & \text{if }\Data\taxonsiteexp =  \ci \\
0, & \text{otherwise.}
\end{dcases}
\end{equation}

\section{Maximum {likelihood} estimation}

Given the likelihood of the data, the ML estimate is simply the value of parameter that maximize this probability.

\section{Bayesian estimation}

The Bayesian Paradigm can be seen as a way to model uncertainty in a probabilistic way .
The probability model that we build can be quite approximate, it reflects one's beliefs and any \gls{prior} experience we may have, it is described as personal or subjective.
So when the uncertainty about the model can be boiled down to a parameter $\theta$, the Bayesian statistician treats $\theta$ as if it were a random variable whose distribution describes that uncertainty.

A subjective probability is going to be subject to modification upon acquisition of further information supplied by experimanetal data.
Suppose a distribution with density $p(\theta)$ describes one's present uncertainties about some probability model with density $p(x|\theta)$.
Those uncertainties will change with the acquisition of data obtained by doing the experiment modelled by $f$.

Bayes theorem is essential in updating :
\begin{equation}
	p(\theta|\data)=\frac{p(\data|\theta)p(\theta)}{p(\data)}
\end{equation}

The probability of H given the data is called the posterior
probability of H, it is \gls{posterior} to the data.
The unconditional probability of $H$ : $p(H)$ is the prior
probability of H.

For given data $p(\data|H)$ is the \gls{likelihood} of H.

For given data we often write :
$p(H|\data) \propto p(\data|H) p(H)$
The \gls{posterior} is proportional to the \gls{likelihood} time the \gls{prior}.

A Bayesian network, Bayes network, belief network, decision network, Bayes(ian) model or probabilistic directed acyclic graphical model is a probabilistic graphical model (a type of statistical model) that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the \gls{likelihood} that any one of several possible known causes was the contributing factor. For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. 

\begin{figure}[htb!]
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=0.6cm and 1.2cm,semithick]
		\tikzstyle{every state}=[]
		
		\node[state] (P) {$\Probmatrix\branchsiteexp$};
		\node[state] (Q) [below right=of P] {$\Submatrix\siteexp$};
		\node[state] (R) [below right=of Q] {$\Mutmatrix$};
		\node[state] (BL) [above right=of P] {$\branchlength\branchexp$};
		\node[state] (Ne) [above right=of Q] {$\omega\siteexp$};
		
		\path 
		(Q) edge [dashed] node [above right] {} (P)
		(BL) edge [dashed] node [] {} (P)
		(Ne) edge [dashed] node {} (Q)
		(R) edge [dashed] node {} (Q);
		\end{tikzpicture}
	\end{center}
	\caption{Directed acyclic graph of dependencies between variables}{Nodes of the directed acyclic graph are the variables, and edges are the functions. Hyper-parameters are depicted in {\color{RED}{red}} circle, random variables in {\color{BLUE}{blue}} circles, and transformed variables in black. Dotted {\color{BLUE}{blue}} line denotes a drawing from a random distribution, and black solid lines denote a function.}
	\label{fig:graph}%
\end{figure}

\subsection{Monte-Carlo Markov-Chain}

The first \acrshort{MC} algorithm is associated with the army laboratory in Los Alamos under the direction of Metropolis in early 1952.
Both a physicist and a mathematician, Nicolas Metropolis was one of the first scientists to work on the Manhattan Project that led to the production of the atomic bomb.
Almost as early, he became obsessed with the hydrogen bomb, which he eventually succeed in building.

Published in June 1953 in the \textit{Journal of Chemical Physics}, the primary focus of \acrshort{MC} algorithm is computing the energy of random configurations for a system of many particles.
This energy is not not available analytically and requires integration across for all realizations of the random configurations of the particle system.
Because the number of dimensions is high (twice the number of particles), numerical integration is impossible using a deterministic algorithm.
Moreover, because the probability of a given configuration can be very small, even Monte Carlo integration, by sampling randomly across the target distribution of configurations fails to correctly approximate this integral.

This problem can however be transposed in the Markov chain realm, where each state of the process is a particular configuration of particles.
The transition probabilities between states must generate a stationary distribution equal to the target distribution of particle configurations.
Given this requirement, and given we can also sample from the transition probabilities, the Monte Carlo Markov chain starts from a given state and can be updated by random sampling from the transition probabilities.
The chain is composed of a period of burn-in, where the current state is of low probability relative to more likely states.
Once reaching the dynamic equilibrium, the energy of each configuration can be computed and the average of this energy is an approximate solution for the integral of energy over the distribution of configurations.


\subsection{Metropolis-Hastings sampling}

The Metropolis algorithm is an acceptance/rejection rule for the probabilities of transition allowing to match the stationary distribution to the specified target distribution, present in the original paper.
From a state $X_t$, the algorithm proceeds as follows at each step of the Markov chain:
\begin{itemize}
	\item Generate a random candidate state $X'$ according to $g(X'\mid X_t)$.
	\item Calculate the acceptance ratio $\displaystyle r=\min \left(1,{\frac {P(X')}{P(X_{t})}}{\frac {g(X_{t}\mid X')}{g(X'\mid X_{t})}}\right)$.
	\item Generate a uniform random number $u\in [0,1]$.
	If $u\leq A(X',X_{t})$, then accept the new state and set $X_{t+1}=X'$.
	Else reject the new state and set $X_{t+1}=X$
\end{itemize}

The algorithm requires the ability to calculate the acceptance ratio $r$ for all possible jump, and to draw a jump from any state. 
In addition, last step above requires the generation of a uniform random number.
The Metropolis procedure has been developped in the context of a symmetric distribution $g(X'\mid X) = g(X \mid X')$, and was later generalized to incorporate any distribution, such that the factor $g(X'\mid X) / g(X \mid X')$ took the name Hasting ratio.

\subsection{Gibbs sampling}

Gibbs sampling is applicable when a joint distribution of variables is not known explicitly or is difficult to sample from directly, but the conditional distribution of each variable are easier to sample from.
The original implementation of the Gibbs sampler was applied to a discrete image processing problem, a problem somewhat removed from statistical inference in the classical sense.
This paper is also responsible for the name {\it Gibbs sampling}, because it implemented this method for the Bayesian study of {\it Gibbs random fields} which, in turn, derive their name from the physicist Josiah Willard Gibbs (1839-1903).

The individual variables are sampled one at a time, with each variable conditioned on the most recent values of all the others.
It can be shown that the sequence of samples constitutes a \gls{mc}, and the stationary distribution of that \gls{mc} is just the joint distribution.
Gibbs sampling is particularly well-adapted to sampling the \gls{posterior} distribution of a Bayesian network, since they are composed of a set of individual random variables in which each variable is conditioned on only a small number of other variables.

Gibbs sampling can be considered a general framework for sampling from a large set of variables by sampling each variable (or in some cases, each group of variables) in turn.
Various algorithms can be used to sample these individual variables, depending on the exact form of the multivariate distribution, it can incorporate the Metropolisâ€“Hastings algorithm, or more sophisticated methods such as slice sampling, adaptive rejection sampling and adaptive rejection Metropolis.

\subsection{Sufficient statistics \& data augmentation}

The basic idea is to augment the observed sequence data with a possible \gls{substitution} history and to then use \gls{mc} Monte Carlo techniques to perform a random walk over histories that are consistent with the observed data.

A realization of the random process results in a detailed \gls{substitution} history over the tree.
Most phylogenetic Monte-Carlo-Markov-Chain (\acrshort{MC}) samplers target the distribution over the model parameters, which means that they have to repeatedly invoke the pruning algorithm to recalculate
the pruning-based \gls{likelihood} which is most often the limiting step of the \acrshort{MC}.

An alternative, which is used here, is to do the \acrshort{MC} conditionally on the detailed \gls{substitution} history $\subhistory$, thus doing the \acrshort{MC} over the augmented configuration~($\subhistory$, $D$), under the target distribution obtained by combining the mapping-based \gls{likelihood} with the \gls{prior} over model parameters

The key idea that makes this strategy efficient is that the mapping-based \gls{likelihood} depends on
compact summary statistics of $\subhistory$ (which in turn depend on the specific parameter component
being resampled), leading to very fast evaluation of the \gls{likelihood}.
On the other hand, this requires to implement more complex \acrshort{MC} procedures, that have to alternate between:

1) sampling $\subhistory$ conditionally on the data and the current parameter configuration.

2) re-sampling the parameters conditionally on $\subhistory$.

To implement the mapping-based \acrshort{MC} sampling strategy, we first sample the detailed \gls{substitution} history $\subhistory$ for all sites along the tree.
Several methods exist for doing this \citep{Nielsen2002,Rodrigue2008}.
Then, we write down the probability of $\subhistory$ given the parameters, and finally, we collect all factors that depend on some parameter of interest and make some simplifications.
This ultimately leads to relatively compact sufficient statistics (Supplementary Materials for the different sufficient statistics used by our model) that are fast to evaluate \citep{Irvahn2014,Davydov2016}.


