\chapter{Probabilistic inference and parameter estimation}
{\hypersetup{linkcolor=GREYDARK}\minitoc}
\label{chap:intro-inference}

The previous chapter treated how \gls{substitution} rates are defined and parameterized in phylogenetic \gls{codon} models, either classical or mechanistic, but not how these parameters are inferred and estimated.
In contrast, the goal of this chapter is to present the methodology for estimating the parameters from a set of observed protein-coding \acrshort{DNA} sequences in different species or lineages.
To do so, I will first introduce the concept of \gls{likelihood} and how the \gls{likelihood} is computed in the context of phylogenetic models (section~\ref{sec-intro:likelihood}).
Then, I will briefly introduce the maximum \gls{likelihood} method of inference (section~\ref{subsec:maximum-likelihood}) and, finally, the principles of Bayesian inference using \gls{Markov chain Monte Carlo} (section~\ref{sec:intro-bayesian}).


\section{Likelihood of the data}
\label{sec-intro:likelihood}

To define the \gls{likelihood}, it is important to realize that \gls{codon} models presented previously can also be used in forward mode, so as to generate a simulated alignment of protein-coding sequences.
Given specific parameter values for the model (generically noted $\theta$), the probability of simulating a replicate of the sequence data exactly identical to the empirical dataset $D$ (noted $\proba (D \mid \theta)$) can then be taken as a measure of how well this alignment is explained by the model, under the specific parameter values $\theta$.
This defines the \gls{likelihood}, which is thus a function of the parameter $\theta$:

\begin{equation}
    L(\theta) = \proba (D \mid \theta)
\end{equation}

Unfortunately, even with an astronomical number of simulations, it is very unlikely to generate precisely our observed alignment, and even more difficult to precisely pin down the probability that our observed alignment has been generated by the model under a given set of parameters.
Deriving this probability analytically is thus the first theoretical question to answer, which is the focus of this section.
The challenge is that only the data for extant species are observed whereas sequence at the root of the tree and subsequent evolutionary events of speciation are not directly observed.
In other words, all possible trajectories leading to the observed alignment must be integrated and weighted by their respective probabilities.

Throughout this development, the tree topology ($\tau$) is considered known and fixed.
This restriction emanates from the fact that the scope of this work is not to infer the topology, but rather the parameters of the molecular evolutionary process.
Moreover, the development conducted below does not delve into the details of how multiple sequence alignments are obtained in practice, and assumes in particular that they are correct.
However, it has been shown that outputs of different sequence alignment methods tend to produce different results that are not always mutually consistent.
The main determining factor of alignment accuracy is evolutionary divergence, such that if alignments are restricted to orthologs from closely related taxa, or to slowly evolving genes, alignment errors become rare and may not cause significant problems.

Importantly, the models of sequence evolution considered in this thesis all assume site independence, such that changes at one sequence position have no impact on whether and how other positions will change.
This assumption of independence between sites allows the probability of an observed alignment to be expressed as the product over alignment columns of the probability of observing each of them.
This independence assumption is a simplification.
However it greatly facilitates likelihood-based inference.

This development of \gls{likelihood} computation is divided into three sections, first integrating over all trajectory along a single branch of the tree (section~\ref{subsec:finite-time-transition-probabilities-over-a-branch-at-a-given-site}),
and subsequently over the entire tree (section~\ref{subsec:integrating-over-ancestral-states}),
while finally efficiently computing the probability of the data given the parameters (section~\ref{subsec:pruning-algorithm}).

\subsection{Finite-time transition probabilities over a branch at a given site}
\label{subsec:finite-time-transition-probabilities-over-a-branch-at-a-given-site}

The point \gls{substitution} process implied by the \gls{codon} model defines the instantaneous rates of change between the different \glspl{codon} through the \gls{substitution} matrix $\Submatrix$.
Given a starting (ancestral) \gls{codon} state and a given amount of time over which the \gls{substitution} process runs, the first task is to derive the probability of the descendant sequence presenting each of the $61$ possible \gls{codon} states.
In practice, the \gls{substitution} rate matrix must be normalized, such that time is measured in units of branch length, expressing the expected number of \gls{neutral} changes that have occurred since the ancestor.
For example, a branch length of 2 implies that 2 changes are expected to be seen on average along the branch under the condition that \glspl{substitution} are \gls{neutral}.
At a given site ($\site$) of the sequence, and along a given branch with branch length $\branchlength$, the \gls{codon} probability matrix $\Probmatrix\siteexp(\branchlength)$ is related to the transition matrix ($\Submatrix\siteexp$ at site $\site$) through the first-order differential equation:
\begin{equation}
    \dfrac{\der \Probmatrix\siteexp(\branchlength)}{\der \branchlength} = \Probmatrix\siteexp(\branchlength) \Submatrix\siteexp,
\end{equation}
which has solution:
\begin{equation}
    \Probmatrix\siteexp(\branchlength) = \e^{\branchlength \Submatrix\siteexp}.
\end{equation}
This integration of the \gls{substitution} rate matrix over the branch takes into account all possible histories of \gls{substitution} events compatible with the states at the two ends, leading to a compact probability matrix computed as an exponential of the rate matrix.
In practice, exponentiating the rate matrix is usually performed using decomposition in eigenvalues and eigenvectors.

\subsection{Integrating over ancestral states}
\label{subsec:integrating-over-ancestral-states}
The challenge for generalizing this argument from a single branch to a complete tree is that only the data at the tips of the tree are observed whereas the states at the internal nodes are not.
If they were known, the \gls{likelihood} would be readily calculated, by taking the product of the transition probabilities over all branches.
As an example, and for better readability, a simple illustrative tree given in figure~\ref{fig:tree} will be used \gls{prior} to giving to general formulas.
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=0.6cm and 1.2cm,semithick]
        \tikzstyle{every state}=[]

        \node[state] (1) [BLUE] {$\state_1$};
        \node[state] (0) [YELLOW, below right=of 1] {$\state_0$};
        \node[state] (2) [BLUE,below left=of 0] {$\state_2$};
        \node[state] (5) [YELLOW,right=of 0] {$\state_5$};
        \node[state] (3) [BLUE,above right=of 5] {$\state_3$};
        \node[state] (4) [BLUE,below right=of 5] {$\state_4$};

        \path[-]
        (1) edge [black] node [above right] {$\branchlength^{(1)}$} (0)
        (2) edge [black] node [below right] {$\branchlength^{(2)}$} (0)
        (5) edge [black] node [above] {$\branchlength^{(5)}$} (0)
        (5) edge [black] node [above left] {$\branchlength^{(3)}$} (3)
        (5) edge [black] node [below left] {$\branchlength^{(4)}$} (4);
    \end{tikzpicture}
    \caption[Illustrative phylogenetic tree]{
    Illustrative phylogenetic tree.
    Internal states of nodes ($\state_0$ and $\state_5$) are represented in yellow, while states of extant nodes ($\state_1$ to $\state_4$) for which the state is known from the observed data is represented in blue.}
    \label{fig:tree}%
\end{figure}

In the example of the illustrated tree, from the observed data for the extant nodes $\Data\siteexp = \{ \state_1,\state_2, \state_3, \state_4 \}$, at site $\site$, given that the states of the internal nodes are known, the \gls{likelihood} is computed as:
\begin{align}
    \proba \left(\Data\siteexp \mid \state_0, \state_5, \Submatrix\siteexp, \branchlength\branchexp \right) & = \Probmatrix\siteexp_{\state_0, \state_1}(\branchlength^{(1)})
    \Probmatrix\siteexp_{\state_0, \state_2}(\branchlength^{(2)}), \\
    & \quad \quad \quad
    \times \Probmatrix\siteexp_{\state_5, \state_3}(\branchlength^{(3)})
    \Probmatrix\siteexp_{\state_5, \state_4}(\branchlength^{(4)})
    \Probmatrix\siteexp_{\state_0, \state_5}(\branchlength^{(5)}). \notag
\end{align}
In the general case of an arbitrary topology with $B$ branches, $\state_{\nodeUp}$ and $\state_{\nodeDown}$ are used to denote the parent and descendant nodes of a branch, the \gls{likelihood} conditional on internal states is given as:
\begin{align}
    \proba \left(\Data\siteexp \mid \state_{I}, \Submatrix\siteexp, \branchlength\branchexp \right) & = \prod\limits_{\branch=1}^{B} \Probmatrix\siteexp_{\state_{\nodeDown}, \state_{\nodeUp}}(\branchlength^{(\branch)}),
\end{align}
where $I$ runs over all the internal nodes, and there are $B$ branches.

Because the states of the internal nodes are actually unknown, the \gls{likelihood} must be summed over all possible configurations for them, including at the root.
At the root, the states are produced according to equilibrium frequencies of the process ($\pi\siteexp$).

In the case of the illustrative example, the total probability is given as:
\begin{align}
    \proba \left(\Data\siteexp \mid \Submatrix\siteexp, \branchlength\branchexp  \right) & = \sum\limits_{\state_0=1}^{61} \subequi_{\state_0} \sum\limits_{\state_5=1}^{61} \subequi_{\state_5} \proba \left(\Data\siteexp \mid \state_0, \state_5, \Submatrix\siteexp, \branchlength\branchexp \right) \\
    & = \sum\limits_{\state_0=1}^{61} \subequi_{\state_0} \sum\limits_{\state_5=1}^{61} \subequi_{\state_5} \Probmatrix\siteexp_{\state_0, \state_1}(\branchlength^{(1)})
    \Probmatrix\siteexp_{\state_0, \state_2}(\branchlength^{(2)}), \\
    & \qquad \qquad \qquad
    \times \Probmatrix\siteexp_{\state_5, \state_3}(\branchlength^{(3)})
    \Probmatrix\siteexp_{\state_5, \state_4}(\branchlength^{(4)})
    \Probmatrix\siteexp_{\state_0, \state_5}(\branchlength^{(5)}).\notag
\end{align}

And because the process is reversible, the \gls{codon} equilibrium frequencies satisfy the equations:
\begin{align}
    \vecZero & = \Subequi\branchsiteexp \Submatrix\branchsiteexp, \\
    \iff \Subequi\branchsiteexp & = \Subequi\branchsiteexp \Probmatrix\branchsiteexp, \\
    \iff \dfrac{\subequi_{\ci}\siteexp}{\subequi_{\cj}\siteexp} & = \dfrac{\submatrix_{\cj, \ci}\siteexp}{\submatrix_{\ci, \cj}\siteexp} \text{ for all pairs }\ci,\cj.
\end{align}

In the general topology with $B$ branches, the \gls{likelihood} is thus given as:
\begin{align}
    \proba \left(\Data\siteexp \mid \Submatrix\siteexp, \branchlength\branchexp \right) & = \sum\limits_{\state_{0}=1}^{61} \subequi_{\state_{0}} \hdots \sum\limits_{\state_{k}=1}^{61} \subequi_{\state_{k}} \proba \left(\Data\siteexp \mid \state_{I}, \Submatrix\siteexp, \branchlength\branchexp \right), \\
    & = \sum\limits_{\state_{0}=1}^{61} \subequi_{\state_{0}} \hdots \sum\limits_{\state_{k}=1}^{61} \subequi_{\state_{k}} \prod\limits_{b=1}^{B} \Probmatrix\siteexp_{\state_{\branch^{+}}, \state_{\branch^{-}}}(\branchlength^{(\branch)}). \label{eq:likelihood-site}
\end{align}

And finally, the assumption of independence between sites allows the probability of an observed set of aligned sequences at the tips of an evolutionary tree to be expressed as the product over alignment columns ($\Nsite$ sites) of the observed nucleotides or amino acids in those columns:
\begin{align}
    \proba \left(\Data \mid \Submatrix\siteexp, \branchlength\branchexp \right) & = \prod\limits_{\site=1}^{\Nsite} \proba \left(\Data\siteexp \mid \Submatrix\siteexp, \branchlength\branchexp \right) \label{eq:likelihood}
\end{align}

\subsection{Pruning algorithm}
\label{subsec:pruning-algorithm}
The \gls{likelihood} at a specific column of a multiple sequence alignment given by equation~\ref{eq:likelihood} requires extensive computation, but can, however, be computed in linear time (as a function of the number of branches) using the pruning algorithm of \citet{Felsenstein1981}.
\begin{align}
    \proba \left(\Data\siteexp \mid \Submatrix\siteexp, \branchlength\branchexp \right) & = \sum\limits_{\ci=1}^{61} \subequi\siteexp_{\ci} \pruning\siteexp_{0} \left( \ci \right),
\end{align}
where $\pruning\siteexp_{\node} \left( \ci \right)$ is computed recursively from the $2$ descendant children $\node_{1}$ and $\node_{2}$ of an internal node $\node$, as:
\begin{align}
    \pruning\siteexp_{\node} \left( \ci \right) =
    \left[ \sum\limits_{\cj=1}^{61}\Probmatrix\siteexp_{\itoj}(\branchlength^{(\node \to \node_{1})}) \pruning\siteexp_{\node_{1}} \left( \cj \right) \right]
    \cdot
    \left[ \sum\limits_{\cj=1}^{61}\Probmatrix\siteexp_{\itoj}(\branchlength^{(\node \to \node_{2})}) \pruning\siteexp_{\node_{2}} \left( \cj \right) \right].
\end{align}
And if the node $\node$ is a node with no descendant, meaning an extant taxa:
\begin{align}
    \pruning\siteexp_{\node} \left( \ci \right) =
    \begin{dcases}
        1, & \text{if } \state_{\node} = \ci \\
        0, & \text{otherwise.}
    \end{dcases}
\end{align}

\subsection{Maximum likelihood}
\label{subsec:maximum-likelihood}

The previous sections introduced the computational procedure to compute the \gls{likelihood}.
Combining this procedure with numerical optimization methods allows one to find the parameter values $\widehat{\theta}$ maximizing the \gls{likelihood}.
In other words, our point estimate for the parameters is taken such as to maximize the probability for the model to reproduce the empirical alignment.
This approach, which enjoys many desirable theoretical properties, such as asymptotic consistency and efficiency, was introduced in phylogenetics by \citet{Cavalli-Sforza1967} with reconstruction based on \gls{allele} frequencies, and then by \citet{Felsenstein1981} for phylogenies based on nucleotide sequences.
It has also been extensively used for estimating the parameters of \gls{codon} models, and in particular, classical $\dnds$ based \gls{codon} models~\citep{Yang1997a,Pond2005,Dutheil2006,Yang2007,Gueguen2013,KosakovskyPond2020}.


\section{Bayesian inference}
\label{sec:intro-bayesian}

An alternative to maximum \gls{likelihood} is Bayesian inference.
This inference methodology, which dates back from Laplace and Bayes (1763), was introduced in phylogenetics by \citet{Yang1997}, \citet{Mau1999}, \citet{Larget1999}, \citet{Li2000} and \citet{Huelsenbeck2001}.
Broadly speaking, the Bayesian paradigm can be seen as a way to model uncertainty in a probabilistic way.
More specifically, the parameters of the model (collectively denoted $\theta$) are considered as random variables, from a \gls{prior} distribution describing our uncertainty about their value before having seen the data.
The probability of those parameters are going to be modified after acquisition of information supplied by observed data.
Formally, this update of our knowledge is captured by the computation of the \gls{posterior} distribution, which is obtained by conditioning the random variable theta on the observed value for the data:
\begin{equation}
    \proba (\theta \mid \Data)=\frac{\proba (\Data \mid \theta)\proba (\theta)}{\proba (\Data)}.
\end{equation}
Here the denominator is the marginal \gls{likelihood}, meaning the \gls{likelihood} integrated over the \gls{prior}:
\begin{equation}
    \proba (\Data) = \int \proba (D \mid \theta) \proba (\theta) \der \theta.
\end{equation}
In an inference context, because we are only interested in the relative \gls{posterior} probabilities of alternative values of $\theta$, the marginal \gls{likelihood} is a constant.
For that reason, Bayes theorem can also be presented as:
\begin{equation}
    \proba (\theta \mid \Data) \propto \proba (\Data \mid \theta)\proba (\theta).
\end{equation}
Simply stating that \gls{posterior} is proportional to \gls{likelihood} multiplied by \gls{prior}.
In other words, updating our knowledge, such as initially represented by our \gls{prior}, is done multiplicatively, using the \gls{likelihood}, and renormalizing to obtain a proper probability distribution (the posterior).

\subsection{Bayesian statistics and model complexity}
\label{subsec:bayesian-statistics-and-maximum-likelihood}
Bayesian statistics and maximum \gls{likelihood} are often opposed to each other and sometimes fiercely defended by their respective proponents.
There are indeed fundamental philosophical differences.
In particular, Bayesian inference is potentially sensitive to the \gls{prior}, although, practically, \gls{prior} sensitivity can be investigated.
In addition \gls{posterior} and \gls{prior} can be presented next to each other, such that differences between the two can be interpreted as the amount of signal extracted from the data, and potential issues with the choice of the \gls{prior} can be pointed out.

However, the recent success of Bayesian inference relates more fundamentally to the way it deals with model complexity~\citep{Huelsenbeck2000a,Lartillot2020}.

First, by sampling from the \gls{posterior} distribution, Bayesian inference offers a method for integrating over the uncertainty about the parameters.
This leads to more robust inference~\citep{Huelsenbeck2000a}.
A corollary is that over parametrization is not such a drastic issue as in maximum \gls{likelihood} inference.
In the worst possible case of over-parametrization, namely that of confounded parameters, such that the model is exactly the same for different set of parameters, confounded parameters can be identified afterward through parameters correlation in their joint \gls{posterior} distribution.
However, over-parameterized models are still a misappropriate use of computing resources, which results in a greater environmental cost.

Second, and most importantly, Bayesian inference gives a natural language to combine multiple levels of random variables, in the form of hierarchical models.
Thus, for instance, in Bayesian molecular dating, the \gls{substitution} process depends on divergence times and \gls{substitution} rate variations across the tree.
In turn, divergence times, such as specified by the phylogeny, are the result of a birth-death process, while variation in the \gls{substitution} rate across branches is naturally expressed by modelling the rate itself as a log-Brownian process.
Finally, the parameters of the birth-death and log-Brownian processes are endowed with a \gls{prior}.
The model is thus hierarchical, with four levels.
This expressiveness in model structure, combined with generic Monte Carlo approaches for dealing with complex random effects and multi-level evolutionary processes, has played a fundamental role in the recent success and popularity of Bayesian inference in evolutionary genetics.

\subsection{Hierarchical model}
\label{subsec:intro-hierarchical-models}
The relationship between the random variables defining a hierarchical model can be formalized as a Bayesian network, which is a probabilistic graphical representation of the set of variables and their conditional dependencies via a directed acyclic graph (DAG).
In the example of the \gls{prior} distribution for the rate \gls{substitution} matrix in the case of the mutation-selection model, the \gls{prior} is defined as the joint distribution of the \gls{prior} over the selection coefficient over amino acids and the mutation rate matrix, which itself is a deterministic function of the equilibrium frequencies of nucleotides and the exchangeability rates for the general-time-reversible (\acrshort{GTR}) mutation matrix (see figure~\ref{fig:DAG}).
Seeing the DAG the other way around (following the arrows), simple \gls{prior} distributions are combined together to form more complex joint \gls{prior} distribution which ultimately defines the \gls{prior} distribution over the model parameter vector ($\theta$).
This hierarchy can naturally be extended across sites, across branches or across genes, and include the data, which are themselves a random variable produced by the \gls{substitution} process.
\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=0.6cm and 1.2cm,semithick]
        \tikzstyle{every state}=[]

        \node[state] (P) {$\Probmatrix\siteexp(\branchlength\branchexp)$};
        \node[state] (Q) [below right=of P] {$\Submatrix\siteexp$};
        \node[state] (BL) [BLUE, above right=of P] {$\branchlength\branchexp$};
        \node[state] (Exp) [RED, right=of BL] {$1$};
        \node[state] (F) [below right=of Q] {$\ScaledFit\siteexp$};
        \node[state] (sb) [BLUE, right=of F] {$\Profile$};
        \node[state] (sbH) [RED, right=of sb] {$\stickbreakinghyper$};
        \node[state] (R) [above right=of Q] {$\Mutmatrix$};
        \node[state] (Ex) [BLUE, above right=of R] {$\Exchan$};
        \node[state] (ExH) [RED, right=of Ex] {$\dfrac{1}{6}, 6$};
        \node[state] (Equi) [BLUE, below right=of R] {$\Mutequi$};
        \node[state] (EquiH) [RED, right=of Equi] {$\dfrac{1}{4}, 4$};

        \path
        (Q) edge [black] node [above right] {} (P)
        (BL) edge [black] node [] {} (P)
        (Exp) edge [dashed, BLUE] node [above] {\textit{Exp}} (BL)
        (R) edge [black] node {} (Q)
        (Ex) edge [black] node [] {} (R)
        (Equi) edge [black] node [below] {} (R)
        (ExH) edge [dashed, BLUE] node [above] {\textit{Dir}} (Ex)
        (EquiH) edge [dashed, BLUE] node [above] {\textit{Dir}} (Equi)
        (F) edge [black] node {} (Q)
        (sb) edge [black] node [above] {} (F)
        (sbH) edge [dashed, BLUE] node [above] {\textit{DP}} (sb);
    \end{tikzpicture}
    \caption[Directed acyclic graph of Bayesian network]{
    Directed acyclic graph of dependencies between variables.
    Nodes of the directed acyclic graph are the variables, and edges are the functions.
    Hyper-parameters are depicted in {\color{RED}{red}} circle, random variables in {\color{BLUE}{blue}} circles, and transformed variables in black.
    {\color{BLUE}{blue}} dashed line denotes a drawing from a random distribution, and black solid lines denote a function.
    \textit{Exp} denotes an exponential distribution, \textit{Dir} denotes a Dirichlet distribution, and finally \textit{DP} denotes a \gls{Dirichlet process}.}
    \label{fig:DAG}
\end{figure}

\subsection{Markov chain Monte Carlo (MCMC)}
\label{subsec:markov-chain-monte-carlo}

Once realizing that the \gls{prior} distribution can be boiled down to a set of simpler distributions over the components of the parameter vector, the difficulty in computing the \gls{posterior} distribution arises from the high dimensionality of the parameter space, known as the curse of dimensionality.
More precisely, the number of states increases exponentially with the number of dimensions of the space, such that the explicit evaluation for both the \gls{prior} and the \gls{likelihood} for a sufficiently fine-grained set of parameter values is unrealistic.
In addition, the \gls{posterior} distribution takes negligibly small values over most of the parameter space.
Reduction in the exploration of the state space, and focusing of most of the computational effort in the relevant region, is obtained by employing Monte Carlo (\acrshort{MC}) methods, which effectively approximate the target \gls{posterior} distribution by sampling from it.

Historically, the first \acrshort{MC} algorithm is associated with the army laboratory in Los Alamos under the direction of Metropolis in early 1952
\footnote{Both a physicist and a mathematician, Nicolas Metropolis was one of the first scientists to work on the Manhattan Project that led to the production of the atomic bomb. Almost as early, he became obsessed with the hydrogen bomb, which he eventually contributed to make.}.
Published by \citet{Metropolis1953}, the primary focus of \acrshort{MC} algorithm is on computing the mean energy of random configurations for a system of many particles.
This energy is not available analytically and requires integration across all realizations of the random configurations of the particle system.
Because dimensionality is high (proportional to the number of particles), numerical integration is impossible using a deterministic algorithm.
Moreover, because the probability of a given configuration can be very small, even Monte Carlo integration by sampling randomly the \gls{prior} (uniform) distribution over configurations fails to correctly approximate this integral.

This problem can, however, be formalized in terms of a \gls{Markov chain}, where each state of the process is a particular configuration of particles.
The transition probabilities between states must generate a stationary distribution equal to the target distribution of particle configurations.
Given this requirement, and given one can also sample from the transition probabilities, the \gls{Markov chain Monte Carlo} starts from an arbitrary state and can be updated by random sampling from the transition probabilities.
After a period of burn-in, the \gls{Markov chain} reaches the dynamic equilibrium, and the energy of each configuration can be computed.
Finally, the average of this energy is an approximate solution for the integral of energy over the thermal equilibrium distribution of atomic configurations.

\subsection{Metropolis-Hastings sampling}
\label{subsec:metropolis-hastings-sampling}

One specific algorithm designed such that the \acrshort{MCMC} stationary distribution match the specified target distribution is the Metropolis algorithm, presented in the original paper~\citep{Metropolis1953}.
This algorithm is composed of an acceptance/rejection rule such that the algorithm proceeds as follows at each step of the \gls{Markov chain}.
Starting from a state $X_t$ at step $t$:
\begin{itemize}
    \item Generate a random candidate state $X'$ according to $g(X'\mid X_t)$.
    \item Calculate the acceptance ratio $\displaystyle r=\min \left(1,{\frac {\proba (X')}{\proba (X_{t})}}{\frac {g(X_{t}\mid X')}{g(X'\mid X_{t})}}\right)$.
    \item Generate a uniform random number $u\in [0,1]$.
    If $u\leq r$, then accept the new state and set $X_{t+1}=X'$.
    Otherwise reject the new state and set $X_{t+1}=X$
\end{itemize}

The algorithm requires the ability to calculate the acceptance ratio $r$ for all possible jump, and to draw a jump from any state.
In addition, the last step above requires the generation of a uniform random number.
The Metropolis procedure has been initially developed in the context of a symmetric distribution $g(X'\mid X) = g(X \mid X')$, and was later generalized to incorporate any proposal distribution, in which case an additional factor named the Hastings ratio ($g(X'\mid X) / g(X \mid X')$) as to be accounted for.

\subsection{Gibbs sampling}
\label{subsec:gibbs-sampling}

Whenever a joint distribution of variables is not known explicitly or is difficult to sample from directly, but the conditional distribution of each variable is easier to sample from, a specific algorithm known as Gibbs sampling is applicable.
The original implementation of the Gibbs sampler by \citet{Geman1984} was applied to a discrete image processing problem, a problem somewhat remote from statistical inference in the classical sense\footnote{This paper is also responsible for the name Gibbs sampling, because it implemented this method for the Bayesian study of Gibbs random fields, which in turn, derive their name from the physicist Josiah Willard Gibbs (1839-1903).}.

The individual random variables are sampled one at a time, with each variable being conditioned on the most recent values for all other variables.
It can be shown that the sequence of samples constitutes a \gls{Markov chain}, and the stationary distribution of that \gls{Markov chain} is just the joint distribution.
Gibbs sampling is particularly well adapted to sampling the \gls{posterior} distribution of a Bayesian network, since they are composed of a set of individual random variables in which each variable is conditioned on only a small number of other variables.

Gibbs sampling, or more generally conditional Metropolis-Hastings can be considered a general framework for sampling from a large set of variables by sampling each variable (or in some cases, each group of variables) in turn.
Various algorithms can be used to sample these individual variables, depending on the exact form of the multivariate distribution, it can incorporate the Metropolis–Hastings algorithm, or more sophisticated methods such as slice sampling, adaptive rejection sampling or adaptive rejection Metropolis.

\subsection{Sufficient statistics \& data augmentation}
\label{subsec:suffstats-data-augmentation}

MCMC samplers target the distribution over the model parameters by repeatedly invoking the pruning algorithm to recalculate the pruning-based \gls{likelihood}.
This is most often the limiting step of the \acrshort{MCMC}.
An alternative is to augment the observed sequence data with a realization of the random process resulting in a detailed \gls{substitution} history over the tree~\citep{Nielsen2002,Rodrigue2008}.
Conditionally on the detailed \gls{substitution} history $\subhistory$, compatible with the data $\Data$, the \acrshort{MC} can be performed over the augmented configuration~($\subhistory, \theta \mid \Data$).
The key idea that makes this strategy efficient is that the mapping-based \gls{likelihood} depends on compact summary statistics of $\subhistory$, leading to very fast evaluation of the likelihood~\citep{Lartillot2006,DeKoning2010,Romiguier2012,Irvahn2014,Davydov2016,Gueguen2018}.
On the other hand, this requires to implement more complex \acrshort{MC} procedures that have to alternate between:
\begin{enumerate}
    \item sampling $\subhistory$ conditionally on the data and the current parameter configuration;
    \item re-sampling the parameters conditionally on $\subhistory$.
\end{enumerate}

This strategy plays an essential role in the case of the complex phylogenetic \gls{codon} model introduced in chapter~\ref{chap:MutSelDrift}.

\subsection{Implementation}
\label{subsec:implementation}

The software implementation of Bayesian phylogenetic models is generally a difficult endeavour.
They must be flexible to adapt to different models of variations, while at the same time be reliable, reproducible, maintainable and fast.
This is even more true for models integrating variation across sites, across branches or across genes.
All these constraints led to the (still ongoing) development of a new Bayesian phylogenetic software platform called \texttt{BayesCode}, conducted by multiple maintainers with different goals and different models of evolution in mind.
\texttt{BayesCode} adopts a modular design, using the graphical model formalism (see section~\ref{subsec:intro-hierarchical-models}) at a coarse-grained level, resulting in a flexible approach for model design by combining building blocks, corresponding to the fundamental distributions, the stochastic processes, and the \gls{likelihood} computation routines that form the basis of a large family of phylogenetic models.
Historically, the development of this software platform was initiated concurrently to the beginning of this thesis, and chapter~\ref{chap:MutSelDrift} which model variation of selection across sites and drift across branches has been implemented under this framework.
This software written in modern \texttt{C++} (version 14) is available at \url{https://github.com/bayesiancook/bayescode}.
