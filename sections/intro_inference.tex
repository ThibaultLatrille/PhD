\chapter{Phylogenetic Bayesian models}
{
	\hypersetup{linkcolor=GREYDARK}
	\minitoc
}
\label{sec:phylo_bayes}

The previous chapter treated how \gls{substitution} rates are defined and parameterized, but not how these parameters are inferred and estimated.
Given a set of observed protein-coding \acrshort{DNA} sequences in different species or lineages, the goal is to estimates the underlying parameters of evolution.
However, models of \glspl{substitution} presented so far are not sufficient to fulfills this endeavor, but instead can be used to generate a random trajectory of protein-coding \acrshort{DNA} sequence evolution.
Indeed, given parameters of evolution, simulations using the \gls{substitution} rates between \glspl{codon}, when run along a phylogenetic tree, can give us a likely representative protein coding \acrshort{DNA} sequence for each extant species (at the tip of the tree), producing an alignment. 
Moreover by running several random simulations, one can expect to obtain representative and likely set of alignments.
Unfortunately, even with huge amount of simulations, it is very unlikely to generate precisely our observed alignment, and even more difficult to precisely pin down the probability of our observed alignment to have been generate by a given set of parameters.
Deriving this probability of observing a specific alignment given the parameters is thus the first theoretical question to answer, which is the focus of the first section.
With this analytical probability obtained, it is then possible the derive for different sets of parameters their respective probability of generating the observed alignment.
The next endeavor is thus to reverse the question, and instead of having the probability of the alignement given the parameters, the goal is to derive the probability of the parameters given the alignment.
This subsequent endeavor is thus the focus of the second section, recalling theoretical and computational implementation of Bayesian inference, in the context of phylogenetic \gls{codon} models.

\section{Likelihood of the data}

For a dataset of protein-coding \acrshort{DNA} sequences, the goal is to determine the probability of the observed sequence data at the tips of the tree conditional upon the evolutionary model, the tree, and values of parameters in the model.
The challenge is that only the data for the extant species are observed whereas the sequence at the root of the tree and the subsequent evolutionary events are not directly observed.
In other words, all possible trajectories leading to the observed alignment must be integrated, weighted by their respective probabilities.

Throughout this development, the tree topology ($\tau$) is considered know and fixed.
This restriction emanates that the scope of this is work is not to infer the topology, but the parameters of evolution.
Moreover, the development does not delves into the details of how alignment are obtained, and thus assume that they are correct.
However it has been shown that outputs of different sequence alignment methods tend to produce different results that are not consistent.
The main determining factor of alignment accuracy is evolutionary divergence, such that if alignment are restricted to orthologs from closely related taxa, or to slow-evolving genes, alignment errors become scarce and may not cause significant encumbrance.
Finally, and not the least, models of sequence change will assume that changes at one sequence position have no impact on whether other positions will change.
This assumption of independence between sites allows the probability of an observed set of aligned sequences at the tips of an evolutionary tree to be expressed as the product over alignment columns of the observed nucleotides or amino acids in those columns. 
This independence assumption is simplistic, throwing away biological information, and can be shown statistically to be problematic, but permits computationally convenient likelihood-based inference.

The section is divided into first integrating over all trajectory over a single branch of tree, and subsequently over the total tree. And finally efficiently computing the probability of observed data given the parameters.

\subsection{Probability of {transitions} for a branch at a given site}
Point \gls{substitution} process define the instantaneous rates of change between the different \glspl{codon} through the \gls{substitution} matrix $\Submatrix$.
However, given a starting state of the process (ancestral) and a given amount of time for which the \gls{substitution} process runs, it is possible to derive the probability of the descendant sequence having each of the $61$ \gls{codon} states.
In practice, time is measured in unit of branch length, expressing the expected number of changes to have occurred since the ancestor if {transition} are \gls{neutral}.
For example of branch length of $2$ imply that $2$ changes are expected to be seen on average along the branch under the condition that \glspl{substitution} are \gls{neutral}.
Consequently, the \gls{substitution} rate matrix must be normalized to be measured in unit of branch length, where in practice instead of normalizing the \gls{codon} subtitution matrix, the nucleotide mutation matrix is normalized since mutation are considered \gls{neutral}.
The mathematical details of this transformation from rate-matrix to probability matrix are described from first order derivation of the Markov process.
At a give site ($\site$) of the sequence, and along a given branch ($\branch$) with branch length $\branchlength\branchexp$, the \gls{codon} probability matrix $\Probmatrix\siteexp(\branchlength\branchexp)$ is related to the {transition} matrix ($\Submatrix\siteexp$ at site $\site$) trough the first-order differential equation:
\begin{align}
	\dfrac{\der \Probmatrix\siteexp(\branchlength\branchexp)}{\der \branchlength\branchexp}	& = \Probmatrix\siteexp(\branchlength\branchexp) \Submatrix\siteexp, \\
	\iff \Probmatrix\siteexp(\branchlength\branchexp) & = \e^{\branchlength\branchexp \Submatrix\siteexp}.
\end{align}
The integration of the \gls{substitution} rate matrix over the branch length entails to take into all possible trajectories histories of evolutionary events, leading to a compact probability matrix computed as an exponential of the rate matrix.
In practive, exponential of the rate matrix is usually performed using eigenvalues and eigenvectors decomposition.

\subsection{Total probability of transition}
The challenge to generalize from branch to tree is that only the data at the tips of the tree are observed whereas the states of the internal nodes are not directly observed.
As a result, \gls{likelihood} of the observed data can be readily calculated if the ancestral state of all internal node is know, from the probabilities of {transitions} along each branch of the tree.
As an example for better readability, a simple illustrative tree given in figure \ref{fig:tree} will be used \gls{prior} to giving to general formulas.
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=0.6cm and 1.2cm,semithick]
	\tikzstyle{every state}=[]

	\node[state] (1) [BLUE] {$\state_1$};
	\node[state] (0) [YELLOW, below right=of 1] {$\state_0$};
	\node[state] (2) [BLUE,below left=of 0] {$\state_2$};
	\node[state] (5) [YELLOW,right=of 0] {$\state_5$};
	\node[state] (3) [BLUE,above right=of 5] {$\state_3$};
	\node[state] (4) [BLUE,below right=of 5] {$\state_4$};

	\path[-]
	(1) edge [black] node [above right] {$\branchlength^{(1)}$} (0)
	(2) edge [black] node [below right] {$\branchlength^{(2)}$} (0)
	(5) edge [black] node [above] {$\branchlength^{(5)}$} (0)
	(5) edge [black] node [above left] {$\branchlength^{(3)}$} (3)
	(5) edge [black] node [below left] {$\branchlength^{(4)}$} (4);
	\end{tikzpicture}
	\caption[Illustrative phylogenetic tree]{
	Illustrative phylogenetic tree. Internal nodes ($\state_0$ and $\state_5$) are represented in yellow, while extant nodes ($\state_1$ to $\state_4$) for which the state is know from the observed data is represented in blue. The node $0$ is considered the root of the tree, although defining is not strictly required since the process is reversible.}
	\label{fig:tree}%
\end{figure}

In the example of the illustrated tree, from the observed data for the extant nodes $\Data\siteexp = \{ \state_1,\state_2, \state_3, \state_4 \}$, at site $\site$, the \gls{likelihood} given that the states of the internal nodes are known is computed as:
\begin{align}
	p\left(\Data\siteexp| \state_0, \state_5, \Submatrix\siteexp, \branchlength\branchexp, \forall \branch \right) & = \Probmatrix\siteexp_{\state_0, \state_1}(\branchlength^{(1)})
	\Probmatrix\siteexp_{\state_0, \state_2}(\branchlength^{(2)}) \\
	& \quad \quad \quad
	\times \Probmatrix\siteexp_{\state_5, \state_3}(\branchlength^{(3)})
	\Probmatrix\siteexp_{\state_5, \state_4}(\branchlength^{(4)})
	\Probmatrix\siteexp_{\state_0, \state_5}(\branchlength^{(5)}) \notag
\end{align}
In the general topology, $\Data\siteexp = \{ \state_P \hdots \state_{2P-1} \}$, where $\state_{\nodeUp}$ and $\state_{\nodeDown}$ are used to denoted the respectively the parent and descendant nodes of a branch, the \gls{likelihood} of the observed data is given as:
\begin{align}
p\left(\Data\siteexp| \state_{I}, \Submatrix\siteexp, \branchlength\branchexp, \forall (I, \branch) \right) & = \prod_{b} \Probmatrix\siteexp_{\state_{\nodeDown}, \state_{\nodeUp}}(\branchlength^{(\branch)}),
\end{align}
where $I$ runs over all the internal nodes.

Because the states of the internal nodes are not actually unknown, the \gls{likelihood} must be summed over the all the possible states, weighted by their equilibrium stationary frequencies ($\pi\siteexp$).

In the case of the illustrative example, the total probability is given as
\begin{align}
	p\left(\Data\siteexp| \Submatrix\siteexp, \branchlength\branchexp , \forall \branch \right) & = \sum\limits_{\state_0=1}^{61} \subequi_{\state_0} \sum\limits_{\state_5=1}^{61} \subequi_{\state_5} p\left(\Data\siteexp| \state_0, \state_5, \Submatrix\siteexp, \branchlength\branchexp \right) \\
	& = \sum\limits_{\state_0=1}^{61} \subequi_{\state_0} \sum\limits_{\state_5}^{61} \subequi_{\state_5} \Probmatrix\siteexp_{\state_0, \state_1}(\branchlength^{(1)})
	\Probmatrix\siteexp_{\state_0, \state_2}(\branchlength^{(2)}) \\
	& \qquad \qquad \qquad
	\times \Probmatrix\siteexp_{\state_5, \state_3}(\branchlength^{(3)})
	\Probmatrix\siteexp_{\state_5, \state_4}(\branchlength^{(4)})
	\Probmatrix\siteexp_{\state_0, \state_5}(\branchlength^{(5)})\notag
\end{align}

And because the process is reversible, the equilibrium frequencies of \glspl{codon} satisfies the equations:
\begin{align}
\bm{0} & = \bm{\pi}\branchsiteexp \Submatrix\branchsiteexp \\
\iff \bm{\pi}\branchsiteexp & = \bm{\pi}\branchsiteexp \Probmatrix\branchsiteexp \\
\iff \dfrac{\subequi_{\ci}\siteexp}{\subequi_{\cj}\siteexp} & = \dfrac{\submatrix_{\cj, \ci}\siteexp}{\submatrix_{\ci, \cj}\siteexp}
\end{align}

In the general topology, the \gls{likelihood} of the observed data is thus given as:
\begin{align}
p\left(\Data\siteexp| \Submatrix\siteexp, \branchlength\branchexp, \forall \branch \right) & = p\left(\Data\siteexp| \state_{I}, \Submatrix\siteexp, \branchlength\branchexp, \forall (I, \branch) \right), \\
& = \sum\limits_{\state_{0}=1}^{61} \subequi_{\state_{0}} \hdots \sum\limits_{\state_{k}=1}^{61} \subequi_{\state_{k}} \prod_{b} \Probmatrix\siteexp_{\state_{\branch^{+}}, \state_{\branch^{-}}}(\branchlength^{(\branch)}), \label{eq:likelihood-site}
\end{align}

And finally, the assumption of independence between sites allows the probability of an observed set of aligned sequences at the tips of an evolutionary tree to be expressed as the product over alignment columns of the observed nucleotides or amino acids in those columns:
\begin{align}
p\left(\Data| \Submatrix\siteexp, \branchlength\branchexp, \forall (\site, \branch) \right) & = \prod_{\site} p\left(\Data\siteexp| \Submatrix\siteexp, \branchlength\branchexp, \forall \branch \right) \label{eq:likelihood}
\end{align} 
\subsection{Pruning algorithm}
The \gls{likelihood} of observed data at a specific column of a multiple sequence alignment, given by equation \ref{eq:likelihood} requires extensive computation, but can however can be determined with the pruning algorithm of \citet{Felsenstein1985}. 
The \gls{likelihood} of the data $\Data$ can be computed using the pruning algorithm:
\begin{align}
p\left(\Data\siteexp| \Submatrix\siteexp, \branchlength\branchexp, \forall \branch \right) & = \sum\limits_{\ci=1}^{61} \subequi\siteexp_{\ci} \pruning\siteexp_{0} \left( \ci \right)
\end{align}
where $\pruning_{\node} \left( \ci \right)$ is computed recursively from the $2$ descendant children $\node_{1}$ and $\node_{2}$ of an internal node $\node$:
\begin{align}
\pruning\siteexp_{\node} \left( \ci \right) = 
\left[ \sum\limits_{\cj=1}^{61}\Probmatrix\siteexp_{\itoj}(\branchlength^{(\node \to \node_{1})}) \pruning\siteexp_{\node_{1}} \left( \cj \right) \right]
\cdot 
\left[ \sum\limits_{\cj=1}^{61}\Probmatrix\siteexp_{\itoj}(\branchlength^{(\node \to \node_{2})}) \pruning\siteexp_{\node_{2}} \left( \cj \right) \right]
\end{align}
And if the node $\node$ is a node with no descendant, meaning an extant taxa:
\begin{align}
\pruning\siteexp_{\node} \left( \ci \right) =
\begin{dcases}
1, & \text{if } \state_{\node} = \ci \\
0, & \text{otherwise.}
\end{dcases}
\end{align}

\section{Bayesian estimation}


$\bullet$ Why has Bayesian statistics been used in phylogenetic inference?

$\bullet$ Difference between likelihood and Bayesian statistics

The Bayesian Paradigm can be seen as a way to model uncertainty in a probabilistic way.
In other words, parameter of the models denoted $\theta$ are considered as random variables whose distribution describes that uncertainty.
This unconditional distribution of parameters, $p(\theta)$, is called the \gls{prior}.
The probability of those parameters are going to be modified after acquisition of information supplied by observed data.
More precisely, Bayes theorem is essential in updating of the distribution :
\begin{equation}
	p(\theta|\data)=\frac{p(\data|\theta)p(\theta)}{p(\data)}
\end{equation}
The probability of the parameters matrix $\theta$ given the data is called the \gls{posterior} probability, it is \gls{posterior} to the data, and $p(\data|\Probmatrix)$ is the 

Because the actual probability of the data is not going to change, in inference Bayes theorem is often presented as:
\begin{equation}
p(\data|\data) \propto p(\data|\data)p(\data)
\end{equation}
Simply stating that the \gls{posterior} is proportional to the \gls{likelihood} time the \gls{prior}.

From the \gls{likelihood} of the data derived in the previous section, the difficulties now is shifted to the definition of the \gls{prior} distribution, where these difficulties come in two flavors.
Firstly, the parameters of evolution for which \gls{prior} distribution can be apprehended might be the parameters of interest such as the selection coefficient, or on the mutation rate, but not the \gls{prior} distribution of the \gls{substitution} rate matrix as a whole.
This difficulties is eased by Bayesian network, which are a probabilistic graphical representation of a set of variables and their conditional dependencies via a directed acyclic graph (DAG).
In the example of the \gls{prior} distribution for the rate \gls{substitution} matrix, it is determined as the joint distribution of the \gls{prior} over the selection coefficient and the mutation rate matrix, which itself is given by the joint distribution over the equilibrium frequencies of nucleotides and exchangeabilities rates for the general-time-reversible (GTR) mutation matrix (see figure \ref{fig:DAG}).
Seeing the DAG the other way around (following the arrows), simple \gls{prior} distribution are combined together to form more complex joint \gls{prior} distribution which ultimately define the \gls{prior} distribution of the model ($\theta$).

\begin{figure}[H]
	\centering
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=0.6cm and 1.2cm,semithick]
	\tikzstyle{every state}=[]
	
	\node[state] (P) {$\Probmatrix\siteexp(\branchlength\branchexp)$};
	\node[state] (Q) [below right=of P] {$\Submatrix\siteexp$};
	\node[state] (BL) [BLUE, above right=of P] {$\branchlength\branchexp$};
	\node[state] (Exp) [RED, right=of BL] {$1$};
	\node[state] (F) [below right=of Q] {$\bm{F}\siteexp$};
	\node[state] (sb) [BLUE, right=of F] {$W$};
	\node[state] (sbH) [RED, right=of sb] {$\stickbreakinghyper$};
	\node[state] (R) [above right=of Q] {$\Mutmatrix$};
	\node[state] (Ex) [BLUE, above right=of R] {$\Exchan$};
	\node[state] (ExH) [RED, right=of Ex] {$\dfrac{1}{6}, 6$};
	\node[state] (Equi) [BLUE, below right=of R] {$\Mutequi$};
	\node[state] (EquiH) [RED, right=of Equi] {$\dfrac{1}{4}, 4$};
	
	\path 
	(Q) edge [black] node [above right] {} (P)
	(BL) edge [black] node [] {} (P)
	(Exp) edge [dashed, BLUE] node [above] {\textit{Exp}} (BL)
	(R) edge [black] node {} (Q)
	(Ex) edge [black] node [] {} (R)
	(Equi) edge [black] node [below] {} (R)
	(ExH) edge [dashed, BLUE] node [above] {\textit{Dir}} (Ex)
	(EquiH) edge [dashed, BLUE] node [above] {\textit{Dir}} (Equi)
	(F) edge [black] node {} (Q)
	(sb) edge [black] node [above] {} (F)
	(sbH) edge [dashed, BLUE] node [above] {\textit{DP}} (sb);
	\end{tikzpicture}
	\caption[Directed acyclic graph of Bayesian network]{
	Directed acyclic graph of dependencies between variables.
	Nodes of the directed acyclic graph are the variables, and edges are the functions. Hyper-parameters are depicted in {\color{RED}{red}} circle, random variables in {\color{BLUE}{blue}} circles, and transformed variables in black.
	{\color{BLUE}{blue}} solid line denotes a drawing from a random distribution, and black solid lines denote a function.
	\textit{Exp} denotes an exponential distribution, \textit{Dir} denotes a Dirichlet distribution, and finally \textit{DP} denotes a \gls{Dirichlet-process}.}
	\label{fig:DAG}
\end{figure}

Secondly, once realizing that the \gls{prior} distribution can be boiled to a set of simpler \gls{prior} distributions, the difficulty arise from the high dimensionality of the parameter space, known as the curse of dimensionality.
More precisely, the number of states increases exponentially with the number of dimension of the space, such that the space is too large to be explored and computation of both the \gls{prior} and the \gls{likelihood} for each set of parameters is unrealistic.
Reduction in the exploration of the state space is obtained by employing Monte-Carlo Markov-Chain, which effectively approximate the target \gls{posterior} distribution.

\subsection{Monte-Carlo Markov-Chain}

The first \acrshort{MC} algorithm is associated with the army laboratory in Los Alamos under the direction of Metropolis in early 1952.
Both a physicist and a mathematician, Nicolas Metropolis was one of the first scientists to work on the Manhattan Project that led to the production of the atomic bomb.
Almost as early, he became obsessed with the hydrogen bomb, which he eventually succeed in building.

Published in June 1953 in the \textit{Journal of Chemical Physics}, the primary focus of \acrshort{MC} algorithm is computing the energy of random configurations for a system of many particles.
This energy is not not available analytically and requires integration across for all realizations of the random configurations of the particle system.
Because the number of dimensions is high (twice the number of particles), numerical integration is impossible using a deterministic algorithm.
Moreover, because the probability of a given configuration can be very small, even Monte Carlo integration, by sampling randomly across the target distribution of configurations fails to correctly approximate this integral.

This problem can however be transposed in the \gls{mc} realm, where each state of the process is a particular configuration of particles.
The {transition} probabilities between states must generate a stationary distribution equal to the target distribution of particle configurations.
Given this requirement, and given we can also sample from the {transition} probabilities, the Monte Carlo \gls{mc} starts from a given state and can be updated by random sampling from the {transition} probabilities.
The chain is composed of a period of burn-in, where the current state is of low probability relative to more likely states.
Once reaching the dynamic equilibrium, the energy of each configuration can be computed and the average of this energy is an approximate solution for the integral of energy over the distribution of configurations.


\subsection{Metropolis-Hastings sampling}

The Metropolis algorithm is an acceptance/rejection rule for the probabilities of {transition} allowing to match the stationary distribution to the specified target distribution, present in the original paper.
From a state $X_t$, the algorithm proceeds as follows at each step of the \gls{mc}:
\begin{itemize}
	\item Generate a random candidate state $X'$ according to $g(X'\mid X_t)$.
	\item Calculate the acceptance ratio $\displaystyle r=\min \left(1,{\frac {P(X')}{P(X_{t})}}{\frac {g(X_{t}\mid X')}{g(X'\mid X_{t})}}\right)$.
	\item Generate a uniform random number $u\in [0,1]$.
	If $u\leq A(X',X_{t})$, then accept the new state and set $X_{t+1}=X'$.
	Else reject the new state and set $X_{t+1}=X$
\end{itemize}

The algorithm requires the ability to calculate the acceptance ratio $r$ for all possible jump, and to draw a jump from any state. 
In addition, last step above requires the generation of a uniform random number.
The Metropolis procedure has been developped in the context of a symmetric distribution $g(X'\mid X) = g(X \mid X')$, and was later generalized to incorporate any distribution, such that the factor $g(X'\mid X) / g(X \mid X')$ took the name Hasting ratio.

\subsection{Gibbs sampling}

Gibbs sampling is applicable when a joint distribution of variables is not known explicitly or is difficult to sample from directly, but the conditional distribution of each variable are easier to sample from.
The original implementation of the Gibbs sampler was applied to a discrete image processing problem, a problem somewhat removed from statistical inference in the classical sense.
This paper is also responsible for the name {\it Gibbs sampling}, because it implemented this method for the Bayesian study of {\it Gibbs random fields} which, in turn, derive their name from the physicist Josiah Willard Gibbs (1839-1903).

The individual variables are sampled one at a time, with each variable conditioned on the most recent values of all the others.
It can be shown that the sequence of samples constitutes a \gls{mc}, and the stationary distribution of that \gls{mc} is just the joint distribution.
Gibbs sampling is particularly well-adapted to sampling the \gls{posterior} distribution of a Bayesian network, since they are composed of a set of individual random variables in which each variable is conditioned on only a small number of other variables.

Gibbs sampling can be considered a general framework for sampling from a large set of variables by sampling each variable (or in some cases, each group of variables) in turn.
Various algorithms can be used to sample these individual variables, depending on the exact form of the multivariate distribution, it can incorporate the Metropolis–Hastings algorithm, or more sophisticated methods such as slice sampling, adaptive rejection sampling and adaptive rejection Metropolis.

\subsection{Sufficient statistics \& data augmentation}

The basic idea is to augment the observed sequence data with a possible \gls{substitution} history and to then use \gls{mc} Monte Carlo techniques to perform a random walk over histories that are consistent with the observed data.

A realization of the random process results in a detailed \gls{substitution} history over the tree.
Most phylogenetic Monte-Carlo-Markov-Chain (\acrshort{MC}) samplers target the distribution over the model parameters, which means that they have to repeatedly invoke the pruning algorithm to recalculate
the pruning-based \gls{likelihood} which is most often the limiting step of the \acrshort{MC}.

An alternative, which is used here, is to do the \acrshort{MC} conditionally on the detailed \gls{substitution} history $\subhistory$, thus doing the \acrshort{MC} over the augmented configuration~($\subhistory$, $\data$), under the target distribution obtained by combining the mapping-based \gls{likelihood} with the \gls{prior} over model parameters

The key idea that makes this strategy efficient is that the mapping-based \gls{likelihood} depends on
compact summary statistics of $\subhistory$ (which in turn depend on the specific parameter component
being resampled), leading to very fast evaluation of the \gls{likelihood}.
On the other hand, this requires to implement more complex \acrshort{MC} procedures, that have to alternate between:

1) sampling $\subhistory$ conditionally on the data and the current parameter configuration.

2) re-sampling the parameters conditionally on $\subhistory$.

To implement the mapping-based \acrshort{MC} sampling strategy, we first sample the detailed \gls{substitution} history $\subhistory$ for all sites along the tree.
Several methods exist for doing this \citep{Nielsen2002,Rodrigue2008}.
Then, we write down the probability of $\subhistory$ given the parameters, and finally, we collect all factors that depend on some parameter of interest and make some simplifications.
This ultimately leads to relatively compact sufficient statistics (Supplementary Materials for the different sufficient statistics used by our model) that are fast to evaluate \citep{Irvahn2014,Davydov2016}.

\subsection{Implementation}

$\bullet$ Modularity of the code, JAGS, MrBayes, RevBayes and now BayesCode,
\url{https://github.com/bayesiancook/bayescode}