\chapter{Probabilistic inference and parameter estimation}
{\hypersetup{linkcolor=GREYDARK}\minitoc}
\label{chap:intro-inference}

The previous chapter treated how substitution rates are defined and parameterized in phylogenetic codon models, either classical or mechanistic, but not how these parameters are inferred and estimated.
In contrast, the goal of this chapter is to present the methodology for estimating the parameters from a set of observed protein-coding \acrshort{DNA} sequences in different species or lineages.
To do so, I will first introduce the concept of likelihood and how the likelihood is computed in the context of phylogenetic models.
Then, I will briefly introduce the maximum likelihood method and, finally, the principles of Bayesian inference using Markov Chain Monte Carlo.


\section{Likelihood of the data}
\label{sec-intro:likelihood}

To define the likelihood, it is important to realize that codon models presented previously can also be used in forward mode, so as to generate a simulated alignment of protein-coding sequences.
Given specific parameter values for the model (generically noted $\theta$), the probability of simulating a replicate of the sequence data exactly identical to the empirical dataset $D$ (noted $\proba (D \mid \theta)$) can then be taken as a measure of how well this alignment is explained by the model, under the specific parameter values $\theta$.
This defines the likelihood, which is thus a function of the parameter $\theta$:

\begin{equation}
    L(\theta) = \proba (D \mid \theta)
\end{equation}

Unfortunately, even with an astronomical number of simulations, it is very unlikely to generate precisely our observed alignment, and even more difficult to precisely pin down the probability that our observed alignment has been generated by the model under a given set of parameters.
Deriving this probability analytically is thus the first theoretical question to answer, which is the focus of this section.
The challenge is that only the data for extant species are observed whereas sequence at the root of the tree and subsequent evolutionary events of speciation are not directly observed.
In other words, all possible trajectories leading to the observed alignment must be integrated and weighted by their respective probabilities.

Throughout this development, the tree topology ($\tau$) is considered known and fixed.
This restriction emanates from the fact that the scope of this work is not to infer the topology, but rather the parameters of the molecular evolutionary process.
Moreover, the development conducted below does not delve into the details of how multiple sequence alignments are obtained in practice, and assumes in particular that they are correct.
However, it has been shown that outputs of different sequence alignment methods tend to produce different results that are not always mutually consistent.
The main determining factor of alignment accuracy is evolutionary divergence, such that if alignments are restricted to orthologs from closely related taxa, or to slowly evolving genes, alignment errors become rare and may not cause significant problems.

Importantly, the models of sequence evolution considered in this thesis all assume site independence, such that changes at one sequence position have no impact on whether and how other positions will change.
This assumption of independence between sites allows the probability of an observed alignment to be expressed as the product over alignment columns of the probability of observing each of them.
This independence assumption is a simplification.
However it greatly facilitates likelihood-based inference.
The section is divided into first integrating over all trajectory along a single branch of the tree, and subsequently over the entire tree, while finally efficiently computing the probability of the data given the parameters.

\subsection{Finite-time transition probabilities over a branch at a given site}

The point substitution process implied by the codon model defines the instantaneous rates of change between the different codons through the substitution matrix $\Submatrix$.
Given a starting (ancestral) codon state and a given amount of time over which the substitution process runs, the first task is to derive the probability of the descendant sequence presenting each of the $61$ possible codon states.
In practice, the substitution rate matrix must be normalized, such that time is measured in units of branch length, expressing the expected number of neutral changes that have occurred since the ancestor.
For example, a branch length of 2 implies that 2 changes are expected to be seen on average along the branch under the condition that substitutions are neutral.
At a given site ($\site$) of the sequence, and along a given branch with branch length $\branchlength$, the codon probability matrix $\Probmatrix\siteexp(\branchlength)$ is related to the transition matrix ($\Submatrix\siteexp$ at site $\site$) through the first-order differential equation:
\begin{equation}
    \dfrac{\der \Probmatrix\siteexp(\branchlength)}{\der \branchlength} = \Probmatrix\siteexp(\branchlength) \Submatrix\siteexp,
\end{equation}
which has solution:
\begin{equation}
    \Probmatrix\siteexp(\branchlength) = \e^{\branchlength \Submatrix\siteexp}.
\end{equation}
This integration of the substitution rate matrix over the branch takes into account all possible histories of substitution events compatible with the states at the two ends, leading to a compact probability matrix computed as an exponential of the rate matrix.
In practice, exponentiating the rate matrix is usually performed using decomposition in eigenvalues and eigenvectors.

\subsection{Integrating over ancestral states}
The challenge for generalizing this argument from a single branch to a complete tree is that only the data at the tips of the tree are observed whereas the states at the internal nodes are not.
If they were known, the likelihood would be readily calculated, by taking the product of the transition probabilities over all branches.
As an example, and for better readability, a simple illustrative tree given in figure~\ref{fig:tree} will be used prior to giving to general formulas.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=0.6cm and 1.2cm,semithick]
        \tikzstyle{every state}=[]

        \node[state] (1) [BLUE] {$\state_1$};
        \node[state] (0) [YELLOW, below right=of 1] {$\state_0$};
        \node[state] (2) [BLUE,below left=of 0] {$\state_2$};
        \node[state] (5) [YELLOW,right=of 0] {$\state_5$};
        \node[state] (3) [BLUE,above right=of 5] {$\state_3$};
        \node[state] (4) [BLUE,below right=of 5] {$\state_4$};

        \path[-]
        (1) edge [black] node [above right] {$\branchlength^{(1)}$} (0)
        (2) edge [black] node [below right] {$\branchlength^{(2)}$} (0)
        (5) edge [black] node [above] {$\branchlength^{(5)}$} (0)
        (5) edge [black] node [above left] {$\branchlength^{(3)}$} (3)
        (5) edge [black] node [below left] {$\branchlength^{(4)}$} (4);
    \end{tikzpicture}
    \caption[Illustrative phylogenetic tree]{
    Illustrative phylogenetic tree.
    Internal nodes ($\state_0$ and $\state_5$) are represented in yellow, while extant nodes ($\state_1$ to $\state_4$) for which the state is known from the observed data is represented in blue.
    The node $0$ is considered the root of the tree, although defining it is not strictly required since the process is reversible.}
    \label{fig:tree}%
\end{figure}

In the example of the illustrated tree, from the observed data for the extant nodes $\Data\siteexp = \{ \state_1,\state_2, \state_3, \state_4 \}$, at site $\site$, given that the states of the internal nodes are known, the likelihood is computed as:
\begin{align}
    \proba \left(\Data\siteexp \mid \state_0, \state_5, \Submatrix\siteexp, \branchlength\branchexp, \forall \branch \right) & = \Probmatrix\siteexp_{\state_0, \state_1}(\branchlength^{(1)})
    \Probmatrix\siteexp_{\state_0, \state_2}(\branchlength^{(2)}) \\
    & \quad \quad \quad
    \times \Probmatrix\siteexp_{\state_5, \state_3}(\branchlength^{(3)})
    \Probmatrix\siteexp_{\state_5, \state_4}(\branchlength^{(4)})
    \Probmatrix\siteexp_{\state_0, \state_5}(\branchlength^{(5)}) \notag
\end{align}
In the general case of an arbitrary topology, $\Data\siteexp = \{ \state_P \hdots \state_{2P-1} \}$, where $\state_{\nodeUp}$ and $\state_{\nodeDown}$ are used to denote the parent and descendant nodes of a branch, the likelihood conditional on internal states is given as:
\begin{align}
    \proba \left(\Data\siteexp \mid \state_{I}, \Submatrix\siteexp, \branchlength\branchexp, \forall (I, \branch) \right) & = \prod_{b} \Probmatrix\siteexp_{\state_{\nodeDown}, \state_{\nodeUp}}(\branchlength^{(\branch)}),
\end{align}
where $I$ runs over all the internal nodes.

Because the states of the internal nodes are actually unknown, the likelihood must be summed over all possible configurations for them, including at the root.
At the root, the states are produced according to equilibrium frequencies of the process ($\pi\siteexp$).

In the case of the illustrative example, the total probability is given as:
\begin{align}
    \proba \left(\Data\siteexp \mid \Submatrix\siteexp, \branchlength\branchexp , \forall \branch \right) & = \sum\limits_{\state_0=1}^{61} \subequi_{\state_0} \sum\limits_{\state_5=1}^{61} \subequi_{\state_5} \proba \left(\Data\siteexp \mid \state_0, \state_5, \Submatrix\siteexp, \branchlength\branchexp \right) \\
    & = \sum\limits_{\state_0=1}^{61} \subequi_{\state_0} \sum\limits_{\state_5}^{61} \subequi_{\state_5} \Probmatrix\siteexp_{\state_0, \state_1}(\branchlength^{(1)})
    \Probmatrix\siteexp_{\state_0, \state_2}(\branchlength^{(2)}) \\
    & \qquad \qquad \qquad
    \times \Probmatrix\siteexp_{\state_5, \state_3}(\branchlength^{(3)})
    \Probmatrix\siteexp_{\state_5, \state_4}(\branchlength^{(4)})
    \Probmatrix\siteexp_{\state_0, \state_5}(\branchlength^{(5)})\notag
\end{align}

And because the process is reversible, the codon equilibrium frequencies satisfy the equations:
\begin{align}
    \bm{0} & = \bm{\pi}\branchsiteexp \Submatrix\branchsiteexp \\
    \iff \bm{\pi}\branchsiteexp & = \bm{\pi}\branchsiteexp \Probmatrix\branchsiteexp \\
    \iff \dfrac{\subequi_{\ci}\siteexp}{\subequi_{\cj}\siteexp} & = \dfrac{\submatrix_{\cj, \ci}\siteexp}{\submatrix_{\ci, \cj}\siteexp}
\end{align}

In the general topology, the likelihood is thus given as:
\begin{align}
    \proba \left(\Data\siteexp \mid \Submatrix\siteexp, \branchlength\branchexp, \forall \branch \right) & = \proba \left(\Data\siteexp \mid \state_{I}, \Submatrix\siteexp, \branchlength\branchexp, \forall (I, \branch) \right), \\
    & = \sum\limits_{\state_{0}=1}^{61} \subequi_{\state_{0}} \hdots \sum\limits_{\state_{k}=1}^{61} \subequi_{\state_{k}} \prod_{b} \Probmatrix\siteexp_{\state_{\branch^{+}}, \state_{\branch^{-}}}(\branchlength^{(\branch)}), \label{eq:likelihood-site}
\end{align}

And finally, the assumption of independence between sites allows the probability of an observed set of aligned sequences at the tips of an evolutionary tree to be expressed as the product over alignment columns of the observed nucleotides or amino acids in those columns:
\begin{align}
    \proba \left(\Data \mid \Submatrix\siteexp, \branchlength\branchexp, \forall (\site, \branch) \right) & = \prod_{\site} \proba \left(\Data\siteexp \mid \Submatrix\siteexp, \branchlength\branchexp, \forall \branch \right) \label{eq:likelihood}
\end{align}

\subsection{Pruning algorithm}
The likelihood at a specific column of a multiple sequence alignment given by equation~\ref{eq:likelihood} requires extensive computation, but can, however, be computed in linear time (as a function of the number of branches) using the pruning algorithm of \citet{Felsenstein1981}.
\begin{align}
    \proba \left(\Data\siteexp \mid \Submatrix\siteexp, \branchlength\branchexp, \forall \branch \right) & = \sum\limits_{\ci=1}^{61} \subequi\siteexp_{\ci} \pruning\siteexp_{0} \left( \ci \right),
\end{align}
where $\pruning_{\node} \left( \ci \right)$ is computed recursively from the $2$ descendant children $\node_{1}$ and $\node_{2}$ of an internal node $\node$:
\begin{align}
    \pruning\siteexp_{\node} \left( \ci \right) =
    \left[ \sum\limits_{\cj=1}^{61}\Probmatrix\siteexp_{\itoj}(\branchlength^{(\node \to \node_{1})}) \pruning\siteexp_{\node_{1}} \left( \cj \right) \right]
    \cdot
    \left[ \sum\limits_{\cj=1}^{61}\Probmatrix\siteexp_{\itoj}(\branchlength^{(\node \to \node_{2})}) \pruning\siteexp_{\node_{2}} \left( \cj \right) \right]
\end{align}
And if the node $\node$ is a node with no descendant, meaning an extant taxa:
\begin{align}
    \pruning\siteexp_{\node} \left( \ci \right) =
    \begin{dcases}
        1, & \text{if } \state_{\node} = \ci \\
        0, & \text{otherwise.}
    \end{dcases}
\end{align}

\subsection{Maximum likelihood}

The previous sections introduced the computational procedure to compute the likelihood.
Combining this procedure with numerical optimization methods allows one to find the parameter values $\widehat{\theta}$ maximizing the likelihood.
In other words, our point estimate for the parameters is taken such as to maximize the probability for the model to reproduce the empirical alignment.
This approach, which enjoys many desirable theoretical properties, such as asymptotic consistency and efficiency, was introduced in phylogenetics by \citet{Cavalli-Sforza1967} with reconstruction based on allele frequencies, and then by \citet{Felsenstein1981} for phylogenies based on nucleotide sequences.
It has also been extensively used for estimating the parameters of codon models, and in particular, classical $\dnds$ based codon models~\citep{Yang1997a,Pond2005,Dutheil2006,Yang2007,Gueguen2013,KosakovskyPond2020}.


\section{Bayesian inference}
\label{sec:intro-bayesian}

An alternative to maximum likelihood is Bayesian inference.
This inference methodology, which dates back from Laplace and Bayes (1763), was introduced in phylogenetics by \citet{Yang1997}, \citet{Mau1999}, \citet{Larget1999}, \citet{Li2000} and \citet{Huelsenbeck2001}.
Broadly speaking, the Bayesian paradigm can be seen as a way to model uncertainty in a probabilistic way.
More specifically, the parameters of the model (collectively denoted $\theta$) are considered as random variables, from a prior distribution describing our uncertainty about their value before having seen the data.
The probability of those parameters are going to be modified after acquisition of information supplied by observed data.
Formally, this update of our knowledge is captured by the computation of the posterior distribution, which is obtained by conditioning the random variable theta on the observed value for the data:
\begin{equation}
    \proba (\theta \mid \data)=\frac{\proba (\data \mid \theta)\proba (\theta)}{\proba (\data)}.
\end{equation}
Where the denominator is the marginal likelihood, meaning the likelihood integrated over the prior:
\begin{equation}
    \proba (\data) = \int \proba (D \mid \theta) \proba (\theta) \der \theta,
\end{equation}
In an inference context, because we are only interested in the relative posterior probabilities of alternative values of $\theta$, the marginal likelihood is a constant.
For that reason, Bayes theorem can also be presented as:
\begin{equation}
    \proba (\theta \mid \data) \propto \proba (\data \mid \theta)\proba (\theta).
\end{equation}
Simply stating that posterior is proportional to likelihood multiplied by prior.
In other words, updating our knowledge, such as initially represented by our prior, is done multiplicatively, using the likelihood, and renormalizing to obtain a proper probability distribution (the posterior).

\subsection{Bayesian statistics and {Maximum likelihood}}
Bayesian statistics and maximum likelihood are often opposed to each other and sometimes fiercely defended by their respective proponents.
There are indeed fundamental philosophical differences.In particular, Bayesian inference is potentially sensitive to the prior, although, practically, prior sensitivity can be investigated.
In addition posterior and prior can be presented next to each other, such that differences between the two can be interpreted as the amount of signal extracted from the data, and potential issues with the choice of the prior can be pointed out.

However, the recent success of Bayesian inference relates more fundamentally to the way it deals with model complexity~\citep{Huelsenbeck2000a,Lartillot2020}.

First, by sampling from the posterior distribution, Bayesian inference offers a method for integrating over the uncertainty about the parameters.
This leads to more robust inference~\citep{Huelsenbeck2000a}.
A corollary is that over parametrization is not such a drastic issue as in maximum likelihood inference.
In the worst possible case of over-parametrization, namely that of confounded parameters, such that the model is exactly the same for different set of parameters, confounded parameters can be identified afterward through parameters correlation in their joint posterior distribution.
However, over-parameterized models are still a misappropriate use of computing resources, which results in a greater environmental cost.

Second, and most importantly, Bayesian inference gives a natural language to combine multiple levels of random variables, in the form of hierarchical models.
Thus, for instance, in Bayesian molecular daring, the substitution process depends on divergence times and substitution rate variation across the tree.
In turn, divergence times, such as specified by the phylogeny, are the result of a birth-death process, while variation in the substitution rate across branches is naturally expressed by modelling the rate itself as a log-Brownian process.
Finally, the parameters of the birth-death and log-Brownian processes are endowed with a prior.
The model is thus hierarchical, with four levels.
This expressivity in model structure, combined with generic Monte Carlo approaches for dealing with complex random effects and multi-level evolutionary processes, has played a fundamental role in the recent success and popularity of Bayesian inference in evolutionary genetics.


\subsection{Hierarchical model}
\label{sec:intro-hierarchical-models}
The relationship between the random variables defining a hierarchical model can be formalized as a Bayesian network, which is a probabilistic graphical representation of the set of variables and their conditional dependencies via a directed acyclic graph (DAG).
In the example of the prior distribution for the rate substitution matrix in the case of the mutation-selection model, the prior is defined as the joint distribution of the prior over the selection coefficient over amino acids and the mutation rate matrix, which itself is a deterministic function of the equilibrium frequencies of nucleotides and the exchangeability rates for the general-time-reversible (\acrshort{GTR}) mutation matrix (see figure~\ref{fig:DAG}).
Seeing the DAG the other way around (following the arrows), simple prior distributions are combined together to form more complex joint prior distribution which ultimately defines the prior distribution over the model parameter vector ($\theta$).
This hierarchy can naturally be extended across sites, across branches or across genes, and include the data, which are themselves a random variable produced by the substitution process.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=0.6cm and 1.2cm,semithick]
        \tikzstyle{every state}=[]

        \node[state] (P) {$\Probmatrix\siteexp(\branchlength\branchexp)$};
        \node[state] (Q) [below right=of P] {$\Submatrix\siteexp$};
        \node[state] (BL) [BLUE, above right=of P] {$\branchlength\branchexp$};
        \node[state] (Exp) [RED, right=of BL] {$1$};
        \node[state] (F) [below right=of Q] {$\bm{F}\siteexp$};
        \node[state] (sb) [BLUE, right=of F] {$\Profile$};
        \node[state] (sbH) [RED, right=of sb] {$\stickbreakinghyper$};
        \node[state] (R) [above right=of Q] {$\Mutmatrix$};
        \node[state] (Ex) [BLUE, above right=of R] {$\Exchan$};
        \node[state] (ExH) [RED, right=of Ex] {$\dfrac{1}{6}, 6$};
        \node[state] (Equi) [BLUE, below right=of R] {$\Mutequi$};
        \node[state] (EquiH) [RED, right=of Equi] {$\dfrac{1}{4}, 4$};

        \path
        (Q) edge [black] node [above right] {} (P)
        (BL) edge [black] node [] {} (P)
        (Exp) edge [dashed, BLUE] node [above] {\textit{Exp}} (BL)
        (R) edge [black] node {} (Q)
        (Ex) edge [black] node [] {} (R)
        (Equi) edge [black] node [below] {} (R)
        (ExH) edge [dashed, BLUE] node [above] {\textit{Dir}} (Ex)
        (EquiH) edge [dashed, BLUE] node [above] {\textit{Dir}} (Equi)
        (F) edge [black] node {} (Q)
        (sb) edge [black] node [above] {} (F)
        (sbH) edge [dashed, BLUE] node [above] {\textit{DP}} (sb);
    \end{tikzpicture}
    \caption[Directed acyclic graph of Bayesian network]{
    Directed acyclic graph of dependencies between variables.
    Nodes of the directed acyclic graph are the variables, and edges are the functions.
    Hyper-parameters are depicted in {\color{RED}{red}} circle, random variables in {\color{BLUE}{blue}} circles, and transformed variables in black.
    {\color{BLUE}{blue}} dashed line denotes a drawing from a random distribution, and black solid lines denote a function.
    \textit{Exp} denotes an exponential distribution, \textit{Dir} denotes a Dirichlet distribution, and finally \textit{DP} denotes a Dirichlet process.}
    \label{fig:DAG}
\end{figure}

\subsection{Markov Chain Monte Carlo (MCMC)}

Once realizing that the prior distribution can be boiled down to a set of simpler distributions over the components of the parameter vector, the difficulty in computing the posterior distribution arises from the high dimensionality of the parameter space, known as the curse of dimensionality.
More precisely, the number of states increases exponentially with the number of dimensions of the space, such that the explicit evaluation for both the prior and the likelihood for a sufficiently fine-grained set of parameter values is unrealistic.
In addition, the posterior distribution takes negligibly small values over most of the parameter space.
Reduction in the exploration of the state space, and focusing of most of the computational effort in the relevant region, is obtained by employing Monte Carlo (\acrshort{MC}) methods, which effectively approximate the target posterior distribution by sampling from it.

Historically, the first \acrshort{MC} algorithm is associated with the army laboratory in Los Alamos under the direction of Metropolis in early 1952.
Both a physicist and a mathematician, Nicolas Metropolis was one of the first scientists to work on the Manhattan Project that led to the production of the atomic bomb.
Almost as early, he became obsessed with the hydrogen bomb, which he eventually contributed to make.

Published by \citet{Metropolis1953}, the primary focus of \acrshort{MC} algorithm is on computing the mean energy of random configurations for a system of many particles.
This energy is not available analytically and requires integration across for all realizations of the random configurations of the particle system.
Because dimensionality is high (proportional to the number of particles), numerical integration is impossible using a deterministic algorithm.
Moreover, because the probability of a given configuration can be very small, even Monte Carlo integration by sampling randomly the prior (uniform) distribution over configurations fails to correctly approximate this integral.

This problem can, however, be formalized in terms of a Markov chain, where each state of the process is a particular configuration of particles.
The transition probabilities between states must generate a stationary distribution equal to the target distribution of particle configurations.
Given this requirement, and given one can also sample from the transition probabilities, the Markov chain Monte-Carlo starts from an arbitrary state and can be updated by random sampling from the transition probabilities.
After a period of burn-in, the Markov chain reaches the dynamic equilibrium, and the energy of each configuration can be computed and the average of this energy is an approximate solution for the integral of energy over the thermal equilibrium distribution of atomic configurations.

\subsection{Metropolis-Hastings sampling}

One specific algorithm designed such that \acrshort{MCMC} stationary distribution match to the specified target distribution is the Metropolis algorithm, presented in the original paper~\citep{Metropolis1953}.
This algorithm is composed of an acceptance/rejection rule such that the algorithm proceeds as follows at each step of the Markov chain.
Starting from a state $X_t$ at step $t$:
\begin{itemize}
    \item Generate a random candidate state $X'$ according to $g(X'\mid X_t)$.
    \item Calculate the acceptance ratio $\displaystyle r=\min \left(1,{\frac {\proba (X')}{\proba (X_{t})}}{\frac {g(X_{t}\mid X')}{g(X'\mid X_{t})}}\right)$.
    \item Generate a uniform random number $u\in [0,1]$.
    If $u\leq A(X',X_{t})$, then accept the new state and set $X_{t+1}=X'$.
    Otherwise reject the new state and set $X_{t+1}=X$
\end{itemize}

The algorithm requires the ability to calculate the acceptance ratio $r$ for all possible jump, and to draw a jump from any state.
In addition, the last step above requires the generation of a uniform random number.
The Metropolis procedure has been initially developed in the context of a symmetric distribution $g(X'\mid X) = g(X \mid X')$, and was later generalized to incorporate any proposal distribution, in which case an additional factor named the Hastings ratio ($g(X'\mid X) / g(X \mid X')$) as to be accounted for.

\subsection{Gibbs sampling}

Whenever a joint distribution of variables is not known explicitly or is difficult to sample from directly, but the conditional distribution of each variable is easier to sample from, a specific algorithm known as Gibbs sampling is applicable.
The original implementation of the Gibbs sampler by \citet{Geman1984} was applied to a discrete image processing problem, a problem somewhat remote from statistical inference in the classical sense.
This paper is also responsible for the name Gibbs sampling, because it implemented this method for the Bayesian study of Gibbs random fields, which in turn, derive their name from the physicist Josiah Willard Gibbs (1839-1903).

The individual random variables are sampled one at a time, with each variable being conditioned on the most recent values for all other variables.
It can be shown that the sequence of samples constitutes a Markov chain, and the stationary distribution of that Markov chain is just the joint distribution.
Gibbs sampling is particularly well adapted to sampling the posterior distribution of a Bayesian network, since they are composed of a set of individual random variables in which each variable is conditioned on only a small number of other variables.

Gibbs sampling, or more generally conditional Metropolis-Hastings can be considered a general framework for sampling from a large set of variables by sampling each variable (or in some cases, each group of variables) in turn.
Various algorithms can be used to sample these individual variables, depending on the exact form of the multivariate distribution, it can incorporate the Metropolisâ€“Hastings algorithm, or more sophisticated methods such as slice sampling, adaptive rejection sampling and adaptive rejection Metropolis.

\subsection{Sufficient statistics \& data augmentation}
\label{subsec:suffstats-data-augmentation}

MCMC samplers target the distribution over the model parameters by repeatedly invoking the pruning algorithm to recalculate the pruning-based likelihood.
This is most often the limiting step of the \acrshort{MCMC}.
An alternative is to augment the observed sequence data with a realization of the random process resulting in a detailed substitution history over the tree~\citep{Nielsen2002,Rodrigue2008}.
Conditionally on the detailed substitution history $\subhistory$, compatible with the data $\Data$, the \acrshort{MC} can be performed over the augmented configuration~($\subhistory, \theta \mid \data$), under the target distribution obtained by combining the mapping-based likelihood with the prior over model parameters.
The key idea that makes this strategy efficient is that the mapping-based likelihood depends on compact summary statistics of $\subhistory$, leading to very fast evaluation of the likelihood~\citep{Lartillot2006,DeKoning2010,Romiguier2012,Irvahn2014,Davydov2016,Gueguen2018}.
On the other hand, this requires to implement more complex \acrshort{MC} procedures that have to alternate between:
\begin{enumerate}
    \item sampling $\subhistory$ conditionally on the data and the current parameter configuration;
    \item re-sampling the parameters conditionally on $\subhistory$.
\end{enumerate}

This strategy plays an essential role in the case of the complex phylogenetic codon model introduced in chapter~\ref{chap:MutSelDrift}.

\subsection{Implementation}
\label{subsec:implementation}

The software implementation of Bayesian phylogenetic models is globally a difficult endeavour.
They must be flexible to adapt to different models of variations, while at the same time be reliable, reproducible, maintainable and fast.
This is even more true for models integrating variation across sites, across branches or across genes.
All these constraints led to the (still ongoing) development of a new Bayesian phylogenetic software platform called \texttt{BayesCode}, conducted by multiple maintainers with different goals and different models of evolution in mind.
\texttt{BayesCode} adopts a modular design, using the graphical model formalism (see~\ref{sec:intro-hierarchical-models}) at a coarse-grained level, resulting in a flexible approach for model design by combining building blocks, corresponding to the fundamental distributions, the stochastic processes, and the likelihood computation routines that form the basis of a large family of phylogenetic models.
Historically, the development of this software platform was initiated concurrently to the beginning of this thesis, and chapter~\ref{chap:MutSelDrift} which model variation of selection across sites and drift across branches has been implemented under this framework.
This software written in modern \texttt{C++} (version 14) is available at \url{https://github.com/bayesiancook/bayescode}.
